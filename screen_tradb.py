#!/usr/bin/env python

import argparse
import pandas as pd
import os
import re
import sqlite3
import subprocess

version_number = "screen_tradb version 0.2"
home_dir = os.path.expanduser('~')


def run_shell_cmd(command):
    """
    Wrapper to run a simple shell command and handle errors.
    
    Args:
        command (str): The shell command to be executed.

    Raises:
        subprocess.CalledProcessError: If the command execution fails.

    Returns:
        None

    Exits:
        Exits the program with status code 1 if the command execution fails.
    """
    try:
        subprocess.run(command, shell=True, check=True)
    except subprocess.CalledProcessError as e:
        print(f"Error executing command: {command}\n{e}")
        exit(1)


def download_and_extract(url, output_dir, output_file, arch_type='tar'):
    """
    Download and extract a file from a URL.

    Args:
        url (str): The URL to download the file from.
        output_dir (str): The directory to extract the file into.
        output_file (str): The name of the output file to save the downloaded content.
        arch_type (str, optional): The type of archive to extract. Defaults to 'tar'.
                                   Supported types are 'tar', 'gz', and None.

    Returns:
        None

    Outputs:
        - The downloaded file is saved to the specified output directory.
        - The file is extracted into the output directory if it is an archive.
    """
    wget_cmd = f'wget {url} -O {output_file}'
    if arch_type == 'tar':
        arch_cmd = f'mkdir -p {output_dir} && tar -xzf {output_file} -C {output_dir}'
    elif arch_type == 'gz':
        arch_cmd = f'mkdir -p {output_dir} && gunzip {output_file}'
    elif arch_type == None:
        arch_cmd = f'echo "metadata database downloaded"'
    run_shell_cmd(wget_cmd)
    run_shell_cmd(arch_cmd)


def run_platon(infile, outdir, basename, num_proc, platon_db_dir):
    """
    Run the Platon tool to predict plasmids from a given input file.

    Args:
        infile (str): Path to the input file to be processed by Platon.
        basename (str): Base name for the output files generated by Platon.
        num_proc (int): Number of processor threads to use for Platon.
        platon_db_dir (str): Directory path where the Platon database is stored.

    Description:
        This function constructs and executes a command to run Platon with specified parameters.
        If the Platon database directory does not exist, it downloads and extracts the database
        from a specified URL. The function then prints the constructed command and executes it
        using the os.system() method.

    Returns:
        None

    Note:
        Attempts to download the platon database from Zenodo if it is not found in the specified directory.

    Outputs:
        - {outdir}/{basename}_platon: Directory containing the output files generated by Platon.
    """
    platon_cmd = f'platon --db {home_dir}/platon_db/db/ --output {outdir}/{basename}_platon --mode accuracy --threads {num_proc} {infile}'

    if not os.path.exists(platon_db_dir):
        download_and_extract('https://zenodo.org/record/4066768/files/db.tar.gz', f'{home_dir}/platon_db', f'{home_dir}/platon_db.tar.gz')

    print(f'Predicting plasmids with platon using the command: {platon_cmd}')
    run_shell_cmd(platon_cmd)
    print(f'Predicted plasmids saved to {outdir}/{basename}_platon/{basename}.plasmid.fasta')


def run_prodigal(infile, basename, outdir):
    """
    Run Prodigal to predict open reading frames (ORFs) and process the output.

    This function executes Prodigal to predict ORFs from a given input file, processes the resulting amino acid sequences
    to replace certain characters, and handles potential issues with Prodigal's DNA output by using bedtools as a workaround.

    Args:
        infile (str): Path to the input file containing nucleotide sequences.
        basename (str): Base name for the output files.
        outdir (str): Directory where the output files will be saved.

    Returns:
        None

    Outputs:
        - {basename}_aa.fa: Amino acid sequences predicted by Prodigal.
        - {basename}.gff: GFF file containing the ORF predictions.
        - {basename}.bed: BED file with corrected coordinates for the ORFs.
        - {basename}_dna.fa: DNA sequences corresponding to the predicted ORFs.
    """
    prodigal_cmd = f'prodigal -i {infile} -p meta -f gff -a {outdir}/{basename}_aa.fa -o {outdir}/{basename}.gff 2> /dev/null'
    run_shell_cmd(prodigal_cmd)
    print("ORF prediction complete")

    pattern = r'>([\w\.]+)_(\d+) # (\d+) # (\d+).+'

    with open(f'{outdir}/{basename}_aa.fa', 'r') as file:
        amino_acids = file.read()
        amino_acids = re.sub(pattern, r'>\1:\3-\4', amino_acids)

    with open(f'{outdir}/{basename}_aa.fa', 'w') as file:
        file.write(amino_acids)

    filtered_lines = []
    with open(f'{outdir}/{basename}.gff', 'r') as gff_file:
        for line in gff_file:
            if not line.startswith("#") and "partial=01" not in line:
                fields = line.strip().split("\t")
                start = str(int(fields[3]) - 1)
                filtered_lines.append((fields[0], start, fields[4]))

    with open(f'{outdir}/{basename}.bed', 'w') as bed_file:
        for entry in filtered_lines:
            bed_file.write("\t".join(entry) + "\n")

    bedtools_cmd = f'bedtools getfasta -fi {infile} -bed {outdir}/{basename}.bed > {outdir}/{basename}_dna.fa'
    run_shell_cmd(bedtools_cmd)


def filter_hits(raw_hits, filtered_hits, identity, coverage, evalue):
    """
    Filters MMseqs2 hits based on identity, coverage, and e-value thresholds, and selects the best hit for each query.

    Args:
        raw_hits (str): Path to the input file containing raw hits in tab-separated format.
        filtered_hits (str): Path to the output file where filtered hits will be saved in tab-separated format.
        identity (float): Minimum percentage identity threshold for filtering hits.
        coverage (float): Minimum query coverage percentage threshold for filtering hits.
        evalue (float): Maximum e-value threshold for filtering hits.

    Returns:
        None

    Outputs:
        - filtered_hits (str): Tab-separated file containing filtered hits of transfer genes.
    """
    identity, coverage, evalue = float(identity), float(coverage), float(evalue)

    hits = pd.read_csv(raw_hits, sep='\t')
    hits = hits[(hits['pident'] >= identity) & (hits['qcov'] >= coverage) & (hits['evalue'] <= evalue)]
    filtered = hits.groupby('query', as_index=False).apply(lambda group: group.loc[group['bits'].idxmax()])
    filtered.rename(columns={'target': 'cluster'}, inplace=True)
    filtered.reset_index(drop=True, inplace=True)
    filtered.to_csv(filtered_hits, sep='\t', index=False)
    print(f'Filtered hits saved to {filtered_hits}')


def get_metadata(filtered_hits, sqlite_db, hit_data):
    """
    This function reads a filtered hits file, retrieves metadata from a SQLite database,
    and merges the metadata with the hits data. The merged data is then saved to a specified file.

    Args:
        filtered_hits (str): Path to the filtered hits file in tab-separated format.
        sqlite_db (str): Path to the SQLite database file.
        hit_data (str): Path to the output file where merged data will be saved in tab-separated format.

    Returns:
        None

    Outputs:
        - hit_data (str): Tab-separated file containing merged data with metadata.

    Description:
        The function performs the following steps:
            1. Reads the filtered hits file into a pandas DataFrame.
            2. Extracts unique clusters from the hits data.
            3. Constructs a SQL query to retrieve metadata (taxa and position) for the clusters from the SQLite database.
            4. Cleans the retrieved metadata by removing unwanted characters from 'taxa' and 'position' columns.
            5. Merges the hits data with the metadata on the 'cluster' column.
            6. Saves the merged data to the specified output file in tab-separated format.
            7. Prints a message indicating the location of the saved metadata.
    """
    hits = pd.read_csv(filtered_hits, sep='\t')
    clusters = hits['cluster'].unique()
    clusters_placeholder = ','.join([f"'{cluster}'" for cluster in clusters])

    conn = sqlite3.connect(sqlite_db)
    query = f"SELECT cluster, taxa, position FROM data WHERE cluster IN ({clusters_placeholder})"
    metadata = pd.read_sql_query(query, conn)
    conn.close()

    metadata['taxa'] = metadata['taxa'].str.replace(r"[\[\]']", '', regex=True)
    metadata['position'] = metadata['position'].str.replace(r"[\[\]']", '', regex=True)

    merged_data = pd.merge(hits, metadata, on='cluster', how='left').drop_duplicates()
    merged_data.to_csv(hit_data, sep="\t", index=False)
    print(f'Metadata saved to {hit_data}')


def filter_fasta(orf_fasta, hit_data, out_fasta):
    """
    Subsets ORFs in FASTA format based on Tra hits and writes the filtered sequences to an output FASTA file.

    Args:
        orf_fasta (str): Path to the input FASTA file containing ORFs.
        hit_data (str): Path to the CSV file containing Tra hit data with 'query' and 'cluster' columns.
        out_fasta (str): Path to the output FASTA file where filtered sequences will be written.

    Returns:
        None

    Outputs:
        - out_fasta (str): FASTA file containing filtered sequences based on Tra hits.

    Description:
        The function performs the following steps:
            1. Reads the ORF FASTA file into a list of tuples (name, sequence).
            2. Reads the hit data into a pandas DataFrame.
            3. Extracts the unique 'query' IDs from the hit data.
            4. Filters the ORF sequences based on the 'query' IDs.
            5. Writes the filtered sequences to the output FASTA file.
            6. Prints a message indicating the location of the saved FASTA file.
    """
    sequences = []
    seq = None
    name = ''
    hits = pd.read_csv(hit_data, sep='\t')
    tra_ids = hits['query'].tolist()

    with open(orf_fasta, 'r') as file:
        for line in file:
            line = line.strip()
            if line.startswith('>'):
                if seq is not None:
                    sequences.append((name, seq))
                name = line[1:]
                seq = ''
            else:
                seq += line
        if seq is not None:
            sequences.append((name, seq))

    filtered = [(f'{", ".join(hits[hits["query"] == name]["cluster"].values)} {name}', seq) for name, seq in sequences if name in tra_ids]

    with open(out_fasta, 'w') as file:
        for name, sequence in filtered:
            file.write(f'>{name}\n{sequence}\n')

    print(f'Filtered sequences saved to {out_fasta}')


def run_mmseqs(seq_db, tra_db_dir, seqs, metadata, basename, outdir, num_proc, identity, coverage, evalue, seq_type):
    """
    Run MMseqs2 to search for sequences in a database and process the results.
    
    Args:
        seq_db (str): Path to the sequence database.
        tra_db_dir (str): Directory where the TraDb database is stored.
        seqs (str): Path to the sequences file.
        metadata (str): Path to the metadata file.
        basename (str): Base name for output files.
        outdir (str): Output directory for results.
        num_proc (int): Number of processors to use.
        identity (float): Minimum sequence identity for filtering hits.
        coverage (float): Minimum coverage for filtering hits.
        evalue (float): E-value threshold for filtering hits.
        seq_type (str): Type of sequences ('dna' or 'protein').

    Returns:
        None

    Outputs:
        - {outdir}/{basename}_{seq_type}_raw_hits.tsv: Raw hits from MMseqs2 search.
        - {outdir}/{basename}_{seq_type}_filtered_hits.tsv: Filtered hits based on criteria.
        - {outdir}/{basename}_{seq_type}_hit_data.tsv: Merged hit data with metadata.
        - {outdir}/{basename}_{seq_type}.fa: Input sequences in FASTA format.
        - {outdir}/{basename}_{seq_type}_hit_sequences.fa: Filtered sequences based on hits.

    Description:
        The function performs the following steps:
            1. Check if the TraDb database directory exists and download the database if it does not.
            2. Run MMseqs2 to search for sequences in the database.
            3. Filter the raw hits based on identity, coverage, and e-value thresholds.
            4. Retrieve metadata for the filtered hits from the metadata database.
            5. Filter the input sequences based on the filtered hits.

            The exact search strategy and filtering criteria are determined by the input parameters and the type of sequences and is set up using argparse.
    """
    raw_hits = f'{outdir}/{basename}_{seq_type}_raw_hits.tsv'
    filtered_hits = f'{outdir}/{basename}_{seq_type}_filtered_hits.tsv'
    hit_data = f'{outdir}/{basename}_{seq_type}_hit_data.tsv'
    orf_fasta = f'{outdir}/{basename}_{seq_type}.fa'
    out_fasta = f'{outdir}/{basename}_{seq_type}_hit_sequences.fa'

    if not os.path.exists(tra_db_dir):
        os.makedirs(tra_db_dir)
    if not os.path.exists(seq_db):
        download_and_extract(f'https://github.com/DEpt-metagenom/TraDb/raw/main/tra_db/Tra_db_sequences_{seq_type}.fa.gz', tra_db_dir, f'{seqs}.gz', arch_type='gz')
        if seq_type == 'dna':
            create_mmseqs_db_cmd = f'mmseqs createdb {seqs} {seq_db} && mmseqs createindex --search-type 3 {seq_db} {tra_db_dir}/tmp'
        else:
            create_mmseqs_db_cmd = f'mmseqs createdb {seqs} {seq_db} && mmseqs createindex {seq_db} {tra_db_dir}/tmp'
        run_shell_cmd(create_mmseqs_db_cmd)
        download_and_extract(f'https://github.com/DEpt-metagenom/TraDb/raw/main/tra_db/Tra_db_taxa_positons_{seq_type}.db', tra_db_dir, metadata, arch_type=None)

    mmseqs_cmd = f'mmseqs easy-search --threads {num_proc} --format-mode 4 --format-output "target,query,pident,nident,qcov,evalue,gapopen,tlen,qlen,bits" --cov-mode 2 -c 0.8 --min-seq-id 0.8 -e 1e-50 {orf_fasta} {seq_db} {raw_hits} tmp 1> {outdir}/{basename}_{seq_type}_mmseqs.log'
    run_shell_cmd(mmseqs_cmd)

    with open(raw_hits, 'r') as fp:
        if sum(1 for line in fp if not line.startswith('target')) == 0:
            print(f"No MMseqs2 hits could be found using {seq_type.upper()} sequences.")
            exit()

    filter_hits(raw_hits, filtered_hits, identity, coverage, evalue)

    with open(filtered_hits, 'r') as fp:
        if sum(1 for line in fp if not line.startswith('target')) == 0:
            print("Filtering removed all hits. Check the table of raw hits and try running again with updated criteria.")
            exit()

    get_metadata(filtered_hits, metadata, hit_data)
    filter_fasta(orf_fasta, hit_data, out_fasta)


def is_dna_sequence(sequence):
    """
    Check if the sequence contains only typical DNA bases.

    Args:
        sequence (str): The DNA sequence to be checked.

    Returns:
        bool: True if the sequence contains only valid DNA bases (A, T, C, G, N, R, Y, S, W, K, M, B, D, H, V), False otherwise.
    """
    dna_bases = set("ATCGNRYSWKMBDHV")
    return all(base in dna_bases for base in sequence.upper())


def parse_fasta(fasta_file):
    """
    Parse a FASTA file manually and return a list of tuples (header, sequence).

    Args:
        fasta_file (str): Path to the FASTA file to be parsed.

    Returns:
        list of tuple: A list where each tuple contains two elements:
            - header (str): The header line of a FASTA entry (without the '>' character).
            - sequence (str): The sequence corresponding to the header.
    """
    with open(fasta_file, 'r') as file:
        sequences = []
        header = None
        sequence = []

        for line in file:
            line = line.strip()

            if line.startswith(">"):
                if header:
                    sequences.append((header, ''.join(sequence)))
                header = line[1:]
                sequence = []
            else:
                sequence.append(line)

        if header:
            sequences.append((header, ''.join(sequence)))

    return sequences


def check_fasta_type(fasta_file):
    """
    Determine whether the sequences in the FASTA file are DNA or Protein.

    Args:
        fasta_file (str): Path to the FASTA file to be checked.

    Returns:
        bool: True if the sequences are DNA, False if they are Protein.
    """
    sequences = parse_fasta(fasta_file)

    for header, sequence in sequences:
        if is_dna_sequence(sequence):
            return True
        else:
            return False


def main():
    """
    Main function that parses command-line arguments and runs the pipeline for screening conjugation transfer genes in plasmid sequences.

    The function performs the following steps:
    1. Parses command-line arguments.
    2. Checks for required dependencies.
    3. Creates the output directory if it does not exist.
    4. Runs the appropriate pipeline based on the input arguments.

    Command-line Arguments:
    - input_fasta (str): Input fasta file. Mandatory input.
    - --type, -t (str): Type of search to be run. Choices are 'dna', 'aa', 'both'. Default is 'both'.
    - --reading_frames, -ORF (bool): If the input consists of ORFs. Default is False.
    - --plasmid_prediction, -p (bool): If plasmid prediction of the input should be attempted. Default is False.
    - --number_of_processors, -np (int): Number of processes invoked for the search. Default is 1.
    - --output_directory, -o (str): Output directory to store results. Default is "tra_out".
    - --platon_db_dir, -pdb (str): Directory where the reference database of platon can be found. Default is $HOME/platon_db.
    - --transfer_gene_db_dir, -tdb (str): Directory where the reference conjugational transfer gene database can be found. Default is $HOME/tra_db.
    - --min_aa_identity, -mai (str): Minimum percent of identity when screening amino acids. Default is 80.
    - --min_aa_coverage, -mac (str): Minimum percent of coverage when screening amino acids. Default is 0.8.
    - --min_aa_evalue, -mae (str): Minimum e-value when screening amino acids. Default is 1e-50.
    - --min_dna_identity, -mdi (str): Minimum percent of identity when screening DNA sequences. Default is 80.
    - --min_dna_coverage, -mdc (str): Minimum percent of coverage when screening DNA sequences. Default is 0.8.
    - --min_dna_evalue, -mde (str): Minimum e-value when screening DNA sequences. Default is 1e-50.
    - --version, -V: Print version number and exit.

    The function also includes basic logging to print the status of the run and checks for the availability of required dependencies.
    """

    print("""
                                          _____          ____  _
           ___  ___ _ __ ___  ___ _ __   |_   _| __ __ _|  _ \| |__
          / __|/ __| '__/ _ \/ _ \ '_ \    | || '__/ _` | | | | '_ \\
          \__ \ (__| | |  __/  __/ | | |   | || | | (_| | |_| | |_) |
          |___/\___|_|  \___|\___|_| |_|   |_||_|  \__,_|____/|_.__/ v0.2
    """)

    parser = argparse.ArgumentParser(description="Screening of conjugation transfer genes in plasmid sequences.")
    parser.add_argument("input_fasta", help="Input fasta file. Mandatory input.")
    parser.add_argument("--type", "-t", default='both', choices=['dna', 'aa', 'both', 'auto'], help="Type of search to be run. Choices are 'dna', 'aa', 'both' and 'auto'. Default is 'both'. Auto works only if the input are ORFs (-ORF or --reading-frames).")
    parser.add_argument("--reading_frames", "-ORF", default=False, action='store_true', help="If the input consists of ORFs. Use this only if you already predicted the ORFs of the input. If set, you should also consider changing --type appropriately. Default is False.")
    parser.add_argument("--plasmid_prediction", "-p", default=False, action='store_true', help="If plasmid prediction of the input should be attempted. The output of platon containing the plasmids will be screened for transfer genes")
    parser.add_argument("--number_of_processors", "-np", type=int, default=1, help="Number of processes invoked for the search.")
    parser.add_argument("--output_directory", "-o", default="tra_out", help="Output directory to store results. Output files will be named using the basename of the input file.")
    parser.add_argument("--platon_db_dir", "-pdb", default=f"{home_dir}/platon_db", help="Where the reference database of platon can be found. Default is $HOME/platon_db. If it cannot be found, automatic install will be attempted.")
    parser.add_argument("--transfer_gene_db_dir", "-tdb", default=f"{home_dir}/tra_db", help="Where the reference conjugational transfer gene database can be found. Default is $HOME/tra_db. If it cannot be found, automatic install will be attempted.")
    parser.add_argument("--min_aa_identity", "-mai", default='80', help="Minimum percent of identity when screening amino acids (0-100). Default is 80.")
    parser.add_argument("--min_aa_coverage", "-mac", default='0.8', help="Minimum percent of coverage when screening amino acids (0-100). Default is 80.")
    parser.add_argument("--min_aa_evalue", "-mae", default='1e-50', help="Minimum e-value when screening amino acids. Default is 1e-50.")
    parser.add_argument("--min_dna_identity", "-mdi", default='80', help="Minimum percent of identity when screening DNA sequences (0-100). Default is 80.")
    parser.add_argument("--min_dna_coverage", "-mdc", default='0.8', help="Minimum percent of coverage when screening DNA sequences (0-100). Default is 80.")
    parser.add_argument("--min_dna_evalue", "-mde", default='1e-50', help="Minimum e-value when screening DNA sequences. Default is 1e-50.")
    parser.add_argument("--version", "-V", action='version', version=version_number, help="Print version number and exit")
    args = parser.parse_args()

    infile = args.input_fasta
    run_type = args.type
    basename = os.path.splitext(os.path.basename(infile))[0]
    outdir = args.output_directory
    input_orf = args.reading_frames
    plasmid_predict = args.plasmid_prediction
    num_proc = args.number_of_processors
    platon_db_dir = args.platon_db_dir
    tra_db_dir = args.transfer_gene_db_dir
    amino_acids_db = f'{tra_db_dir}/Tra_db_sequences_aa'
    tra_amino_acids = f'{amino_acids_db}.fa'
    tra_amino_data = f'{tra_db_dir}/Tra_db_taxa_positons_aa.db'
    mai = args.min_aa_identity
    mac = args.min_aa_coverage
    mae = args.min_aa_evalue
    dna_db = f'{tra_db_dir}/Tra_db_sequences_dna'
    tra_dna = f'{dna_db}.fa'
    tra_dna_data = f'{tra_db_dir}/Tra_db_taxa_positons_dna.db'
    mdi = args.min_dna_identity
    mdc = args.min_dna_coverage
    mde = args.min_dna_evalue

    if not os.path.exists(infile):
        print(f"Input file {infile} not found. Please check the path and try again.")
        exit()

    print(f'Reading input file: {args.input_fasta}')
    print(f'Type of search is set to: {args.type}')
    print(f'The run will use {args.number_of_processors} threads')
    print(f'Output will be saved to {args.output_directory}')
    print(f'Output prefix will be: {basename}')

    dependencies = ['prodigal', 'bedtools', 'platon', 'wget', 'mmseqs']

    for dep in dependencies:
        if subprocess.run(f'which {dep}', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE).returncode != 0:
            print(f'{dep} is not available')
            exit()

    if input_orf and plasmid_predict:
        print("--reading_frames and --plasmid_prediction are both set to True. Check your input and run screen_tradb.py again.")
        exit()

    if run_type == "auto" and not input_orf:
        print("Auto mode can only be used if the input is ORFs. Please set -ORF or --reading-frames to True and run again or specify the type of search to be run ('dna', 'aa' or 'both').")
        exit()

    if not os.path.exists(outdir):
        os.makedirs(outdir)

    if not input_orf and not plasmid_predict:
        run_prodigal(infile, basename, outdir)
        if run_type in ["both", "dna"]:
            run_mmseqs(dna_db, tra_db_dir, tra_dna, tra_dna_data, basename, outdir, num_proc, mdi, mdc, mde, "dna")
        if run_type in ["both", "aa"]:
            run_mmseqs(amino_acids_db, tra_db_dir, tra_amino_acids, tra_amino_data, basename, outdir, num_proc, mai, mac, mae, "aa")
    elif input_orf and not plasmid_predict:
        if run_type == "auto":
            if check_fasta_type(infile):
                print("Input file most probably contains DNA sequences.")
                run_shell_cmd(f'cp {infile} {outdir}/{basename}_dna.fa')
                run_mmseqs(dna_db, tra_db_dir, tra_dna, tra_dna_data, basename, outdir, num_proc, mdi, mdc, mde, "dna")
            else:
                print("Input file most probably contains amino acid sequences.")
                run_shell_cmd(f'cp {infile} {outdir}/{basename}_aa.fa')
                run_mmseqs(amino_acids_db, tra_db_dir, tra_amino_acids, tra_amino_data, basename, outdir, num_proc, mai, mac, mae, "aa")
        else:
            if run_type == "dna":
                if not check_fasta_type(infile):
                    print("Input file does not seem to contain DNA sequences. Please check the input and try again.")
                    exit()
                run_shell_cmd(f'cp {infile} {outdir}/{basename}_dna.fa')
                run_mmseqs(dna_db, tra_db_dir, tra_dna, tra_dna_data, basename, outdir, num_proc, mdi, mdc, mde, "dna")
            if run_type == "aa":
                if check_fasta_type(infile):
                    print("Input file does not seem to contain amino acid sequences. Please check the input and try again.")
                    exit()
                run_shell_cmd(f'cp {infile} {outdir}/{basename}_aa.fa')
                run_mmseqs(amino_acids_db, tra_db_dir, tra_amino_acids, tra_amino_data, basename, outdir, num_proc, mai, mac, mae, "aa")
            else:
                if run_type not in ["dna", "aa"]:
                    print("Please specify the type of search to be run. Choices of --type (-t) are 'dna', 'aa' if screen_tradb.py is run with ORFs as input setting this is mandatory. \nRunning -t auto could be attempted to determine the input type automatically.")
    elif not input_orf and plasmid_predict:
        run_platon(infile, outdir, basename, num_proc, platon_db_dir)
        prodigal_infile = f'{outdir}/{basename}_platon/{basename}.plasmid.fasta'
        run_prodigal(prodigal_infile, basename, outdir)
        if run_type in ["both", "dna"]:
            run_mmseqs(dna_db, tra_db_dir, tra_dna, tra_dna_data, basename, outdir, num_proc, mdi, mdc, mde, "dna")
        if run_type in ["both", "aa"]:
            run_mmseqs(amino_acids_db, tra_db_dir, tra_amino_acids, tra_amino_data, basename, outdir, num_proc, mai, mac, mae, "aa")

if __name__ == "__main__":
    main()
